{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3puLtSfFJ+4+Ai9saxOoi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/DL_Theory/blob/main/DL_Assignment_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
        "\n",
        "**Ans:** No,The weights attached to the same neuron, continue to remain the same throughout the training. It makes the hidden units symmetric hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value."
      ],
      "metadata": {
        "id": "53gQKrkXJS6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\tIs it okay to initialize the bias terms to 0?** \n",
        "\n",
        "**Ans:** It is possible to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights."
      ],
      "metadata": {
        "id": "yPUHetURJRia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tName three advantages of the ELU activation function over ReLU.**\n",
        "\n",
        "**Ans:** \n",
        "* ELU becomes smooth slowly until its output equal to -Î± whereas RELU sharply smoothes.\n",
        "* ELU is a strong alternative to ReLU.\n",
        "* Unlike to ReLU, ELU can produce negative outputs\n"
      ],
      "metadata": {
        "id": "Q3a11Vg2JQG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "* The ELU function is a good default.\n",
        "\n",
        "* If you need the network to be as fast as possible, you can use one of the leaky ReLU variants.\n",
        "\n",
        "* The basic ReLU function is also preferred due to its simplicity, despite the fact they are generally outperformed by ELU and leaky ReLU. If you have a situation where it's preferable to have neurons output exactly 0, then ReLU is a good choice.\n",
        "\n",
        "* tanh can be useful in the output layer if you need your outputs to be between -1 and 1. It's not used too much in hidden layers.\n",
        "\n",
        "* The logistic function is useful in the output layer when you need to estimate a probability like in the binary, multi-class, or multi-class multi-label classification problems. Like tanh, it is not used in the hidden layers.\n",
        "\n",
        "* The softmax function is useful in the output layer when you need to output probabilities for mutually exclusive classes. Again, it's not used in hidden layers."
      ],
      "metadata": {
        "id": "FbF4QiieJOqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?**\n",
        "\n",
        "**Ans:**The purpose of the momentum hyperparameter is to simulate friction if  set it to 0 for high friction, and 1 for no friction.\n",
        "Hyperparameter gets closer to 1, there will be less friction this means that the optimizer will overshoot, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons why it is good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence."
      ],
      "metadata": {
        "id": "p8H4pJLPJMKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.\tName three ways you can produce a sparse model.**\n",
        "\n",
        "**Ans:**A sparse model is that where most weights are equal to 0. There's a couple ways of achieving that effect.\n",
        "\n",
        "* You can train the model normally then zero out tiny weights.\n",
        "\n",
        "* For more sparsity, you can apply l1 regularization during training, which pushes the optimizer towards sparsity.\n",
        "\n",
        "* Finally, you can combine l1 regulatization with dual averaging using TensorFlow's FTRLOptimizer class.\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZPJzUPKJKRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?**\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Dropout is a popular regularization technique for deep neural networks.\n",
        "\n",
        "* The algorithm is: at each training step, every neuron (including the input neurons but excluding the output neurons) has a probability p of being temporariliy \"dropped out\", meaning it will be entierly ignored during this training step. However, it may be active during the next step.\n",
        "\n",
        "* The hyperparameter p is called the dropout rate, and it is typically set to 0.5.\n",
        "\n",
        "* After training the neurons don't get dropped anymore. That's the gist of the algorithm.\n",
        "\n",
        "* Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. Remember, because dropout is only tuned during training, it has no impact on inference."
      ],
      "metadata": {
        "id": "maVs11JTJI0q"
      }
    }
  ]
}